{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import warnings\n",
    "from math import sqrt\n",
    "warnings.filterwarnings('ignore')\n",
    "from azureml.core.run import Run\n",
    "from azureml.core.experiment import Experiment\n",
    "from azureml.core.workspace import Workspace\n",
    "from azureml.core.model import Model\n",
    "from azureml.core.authentication import ServicePrincipalAuthentication\n",
    "from azureml.train.automl import AutoMLConfig\n",
    "import pickle\n",
    "from matplotlib import pyplot as plt\n",
    "from matplotlib.pyplot import figure\n",
    "import mlflow\n",
    "from mlflow.tracking import MlflowClient\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core import Workspace, Dataset\n",
    "\n",
    "subscription_id = 'xxxxxx-xxxxx-xxxxxx-xxxxxx'\n",
    "resource_group = 'Learn_MLOps'\n",
    "workspace_name = 'MLOps_WS'\n",
    "\n",
    "#workspace = Workspace(subscription_id, resource_group, workspace_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'workspace' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_66619/1003836198.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0muri\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mworkspace\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_mlflow_tracking_uri\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mmlflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_tracking_uri\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muri\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'workspace' is not defined"
     ]
    }
   ],
   "source": [
    "uri = workspace.get_mlflow_tracking_uri()\n",
    "mlflow.set_tracking_uri(uri)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'workspace' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_66619/2143699408.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Importing pre-processed dataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_by_name\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mworkspace\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'processed_weather_data_portofTurku'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mversion\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'workspace' is not defined"
     ]
    }
   ],
   "source": [
    "# Importing pre-processed dataset\n",
    "dataset = Dataset.get_by_name(workspace, name='processed_weather_data_portofTurku')\n",
    "print(dataset.name, dataset.version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'dataset' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_66619/4080714342.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_pandas_dataframe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'dataset' is not defined"
     ]
    }
   ],
   "source": [
    "df = dataset.to_pandas_dataframe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Timestamp</th>\n",
       "      <th>Location</th>\n",
       "      <th>Temperature_C</th>\n",
       "      <th>Humidity</th>\n",
       "      <th>Wind_speed_kmph</th>\n",
       "      <th>Wind_bearing_degrees</th>\n",
       "      <th>Visibility_km</th>\n",
       "      <th>Pressure_millibars</th>\n",
       "      <th>Weather_conditions</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2006-04-01 00:00:00+02:00</td>\n",
       "      <td>Port of Turku, Finland</td>\n",
       "      <td>9.472222</td>\n",
       "      <td>0.89</td>\n",
       "      <td>14.1197</td>\n",
       "      <td>251</td>\n",
       "      <td>15.8263</td>\n",
       "      <td>1015.13</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2006-04-01 01:00:00+02:00</td>\n",
       "      <td>Port of Turku, Finland</td>\n",
       "      <td>9.355556</td>\n",
       "      <td>0.86</td>\n",
       "      <td>14.2646</td>\n",
       "      <td>259</td>\n",
       "      <td>15.8263</td>\n",
       "      <td>1015.63</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2006-04-01 02:00:00+02:00</td>\n",
       "      <td>Port of Turku, Finland</td>\n",
       "      <td>9.377778</td>\n",
       "      <td>0.89</td>\n",
       "      <td>3.9284</td>\n",
       "      <td>204</td>\n",
       "      <td>14.9569</td>\n",
       "      <td>1015.94</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2006-04-01 03:00:00+02:00</td>\n",
       "      <td>Port of Turku, Finland</td>\n",
       "      <td>8.288889</td>\n",
       "      <td>0.83</td>\n",
       "      <td>14.1036</td>\n",
       "      <td>269</td>\n",
       "      <td>15.8263</td>\n",
       "      <td>1016.41</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2006-04-01 04:00:00+02:00</td>\n",
       "      <td>Port of Turku, Finland</td>\n",
       "      <td>8.755556</td>\n",
       "      <td>0.83</td>\n",
       "      <td>11.0446</td>\n",
       "      <td>259</td>\n",
       "      <td>15.8263</td>\n",
       "      <td>1016.51</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   Timestamp                Location  Temperature_C  Humidity  \\\n",
       "0  2006-04-01 00:00:00+02:00  Port of Turku, Finland       9.472222      0.89   \n",
       "1  2006-04-01 01:00:00+02:00  Port of Turku, Finland       9.355556      0.86   \n",
       "2  2006-04-01 02:00:00+02:00  Port of Turku, Finland       9.377778      0.89   \n",
       "3  2006-04-01 03:00:00+02:00  Port of Turku, Finland       8.288889      0.83   \n",
       "4  2006-04-01 04:00:00+02:00  Port of Turku, Finland       8.755556      0.83   \n",
       "\n",
       "   Wind_speed_kmph  Wind_bearing_degrees  Visibility_km  Pressure_millibars  \\\n",
       "0          14.1197                   251        15.8263             1015.13   \n",
       "1          14.2646                   259        15.8263             1015.63   \n",
       "2           3.9284                   204        14.9569             1015.94   \n",
       "3          14.1036                   269        15.8263             1016.41   \n",
       "4          11.0446                   259        15.8263             1016.51   \n",
       "\n",
       "   Weather_conditions  \n",
       "0                   0  \n",
       "1                   0  \n",
       "2                   0  \n",
       "3                   0  \n",
       "4                   0  "
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df=pd.read_csv(\"./Dataset/weather_dataset_processed.csv\")\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spliting Pre-Processed data into Training and Validation datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validation set is used later to evaluate model performance post training. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_training = df.iloc[:77160]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(77160, 9)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_training.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_validation = df.drop(df_training.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(19293, 9)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_validation.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Registering Training and Validation data to the datastore on the workspace. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mkdir: cannot create directory ‘Data’: File exists\r\n"
     ]
    }
   ],
   "source": [
    "!mkdir Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_training.to_csv('Dataset/training_data.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_validation.to_csv('Dataset/validation_data.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'workspace' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_66619/2539429335.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdatastore\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mworkspace\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_default_datastore\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'workspace' is not defined"
     ]
    }
   ],
   "source": [
    "datastore = workspace.get_default_datastore()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'datastore' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_66619/3454785533.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdatastore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc_dir\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'Dataset'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_path\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'data'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'datastore' is not defined"
     ]
    }
   ],
   "source": [
    "datastore.upload(src_dir='Dataset', target_path='data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'datastore' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_66619/4218163464.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtraining_dataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTabular\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_delimited_files\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdatastore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'data/training_data.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'datastore' is not defined"
     ]
    }
   ],
   "source": [
    "training_dataset = Dataset.Tabular.from_delimited_files(datastore.path('data/training_data.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'datastore' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_66619/1320541362.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mvalidation_dataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTabular\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_delimited_files\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdatastore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'data/validation_data.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'datastore' is not defined"
     ]
    }
   ],
   "source": [
    "validation_dataset = Dataset.Tabular.from_delimited_files(datastore.path('data/validation_data.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'training_dataset' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_66619/2676607124.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m training_ds = training_dataset.register(workspace=workspace,\n\u001b[0m\u001b[1;32m      2\u001b[0m                                  \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'training_dataset'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m                                  description='Dataset to use for ML training')\n",
      "\u001b[0;31mNameError\u001b[0m: name 'training_dataset' is not defined"
     ]
    }
   ],
   "source": [
    "training_ds = training_dataset.register(workspace=workspace,\n",
    "                                 name='training_dataset',\n",
    "                                 description='Dataset to use for ML training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'validation_dataset' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_66619/2781208120.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m validation_ds = validation_dataset.register(workspace=workspace,\n\u001b[0m\u001b[1;32m      2\u001b[0m                                  \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'validation_dataset'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m                                  description='Dataset for validation ML models')\n",
      "\u001b[0;31mNameError\u001b[0m: name 'validation_dataset' is not defined"
     ]
    }
   ],
   "source": [
    "validation_ds = validation_dataset.register(workspace=workspace,\n",
    "                                 name='validation_dataset',\n",
    "                                 description='Dataset for validation ML models')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data ingestion step - Training dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'workspace' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_66619/2667728953.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_by_name\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mworkspace\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'training_dataset'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mversion\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'workspace' is not defined"
     ]
    }
   ],
   "source": [
    "dataset = Dataset.get_by_name(workspace, name='training_dataset')\n",
    "print(dataset.name, dataset.version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'dataset' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_66619/4080714342.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_pandas_dataframe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'dataset' is not defined"
     ]
    }
   ],
   "source": [
    "df = dataset.to_pandas_dataframe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Timestamp</th>\n",
       "      <th>Location</th>\n",
       "      <th>Temperature_C</th>\n",
       "      <th>Humidity</th>\n",
       "      <th>Wind_speed_kmph</th>\n",
       "      <th>Wind_bearing_degrees</th>\n",
       "      <th>Visibility_km</th>\n",
       "      <th>Pressure_millibars</th>\n",
       "      <th>Weather_conditions</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2006-04-01 00:00:00+02:00</td>\n",
       "      <td>Port of Turku, Finland</td>\n",
       "      <td>9.472222</td>\n",
       "      <td>0.89</td>\n",
       "      <td>14.1197</td>\n",
       "      <td>251</td>\n",
       "      <td>15.8263</td>\n",
       "      <td>1015.13</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2006-04-01 01:00:00+02:00</td>\n",
       "      <td>Port of Turku, Finland</td>\n",
       "      <td>9.355556</td>\n",
       "      <td>0.86</td>\n",
       "      <td>14.2646</td>\n",
       "      <td>259</td>\n",
       "      <td>15.8263</td>\n",
       "      <td>1015.63</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2006-04-01 02:00:00+02:00</td>\n",
       "      <td>Port of Turku, Finland</td>\n",
       "      <td>9.377778</td>\n",
       "      <td>0.89</td>\n",
       "      <td>3.9284</td>\n",
       "      <td>204</td>\n",
       "      <td>14.9569</td>\n",
       "      <td>1015.94</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2006-04-01 03:00:00+02:00</td>\n",
       "      <td>Port of Turku, Finland</td>\n",
       "      <td>8.288889</td>\n",
       "      <td>0.83</td>\n",
       "      <td>14.1036</td>\n",
       "      <td>269</td>\n",
       "      <td>15.8263</td>\n",
       "      <td>1016.41</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2006-04-01 04:00:00+02:00</td>\n",
       "      <td>Port of Turku, Finland</td>\n",
       "      <td>8.755556</td>\n",
       "      <td>0.83</td>\n",
       "      <td>11.0446</td>\n",
       "      <td>259</td>\n",
       "      <td>15.8263</td>\n",
       "      <td>1016.51</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   Timestamp                Location  Temperature_C  Humidity  \\\n",
       "0  2006-04-01 00:00:00+02:00  Port of Turku, Finland       9.472222      0.89   \n",
       "1  2006-04-01 01:00:00+02:00  Port of Turku, Finland       9.355556      0.86   \n",
       "2  2006-04-01 02:00:00+02:00  Port of Turku, Finland       9.377778      0.89   \n",
       "3  2006-04-01 03:00:00+02:00  Port of Turku, Finland       8.288889      0.83   \n",
       "4  2006-04-01 04:00:00+02:00  Port of Turku, Finland       8.755556      0.83   \n",
       "\n",
       "   Wind_speed_kmph  Wind_bearing_degrees  Visibility_km  Pressure_millibars  \\\n",
       "0          14.1197                   251        15.8263             1015.13   \n",
       "1          14.2646                   259        15.8263             1015.63   \n",
       "2           3.9284                   204        14.9569             1015.94   \n",
       "3          14.1036                   269        15.8263             1016.41   \n",
       "4          11.0446                   259        15.8263             1016.51   \n",
       "\n",
       "   Weather_conditions  \n",
       "0                   0  \n",
       "1                   0  \n",
       "2                   0  \n",
       "3                   0  \n",
       "4                   0  "
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(96453, 9)"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Feature Selection and scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, ..., 1, 1, 1])"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = df[['Temperature_C', 'Humidity', 'Wind_speed_kmph', 'Wind_bearing_degrees', 'Visibility_km', 'Pressure_millibars', 'Weather_conditions']].values\n",
    "y = df['Weather_conditions'].values\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting the Training dataset into Train and Test set for ML training\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "sc = StandardScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = sc.fit_transform(X_train)\n",
    "X_test = sc.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model training and Testing Step"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Support Vector Machine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'workspace' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_66619/1092656921.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmyexperiment\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mExperiment\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mworkspace\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"support-vector-machine\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mmlflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_experiment\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"mlflow-support-vector-machine\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'workspace' is not defined"
     ]
    }
   ],
   "source": [
    "myexperiment = Experiment(workspace, \"support-vector-machine\")\n",
    "mlflow.set_experiment(\"mlflow-support-vector-machine\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from sklearn.svm import SVC\n",
    "from sklearn import svm, datasets\n",
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = {'kernel':('linear', 'rbf'), 'C':[1, 10]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "svc = svm.SVC()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize a run in Azureml and mlflow experiments\n",
    "run = myexperiment.start_logging()\n",
    "mlflow.start_run()\n",
    "\n",
    "\n",
    "run.log(\"dataset name\", dataset.name)\n",
    "run.log(\"dataset Version\", dataset.version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "svc_grid = GridSearchCV(svc, parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 6min 40s, sys: 1.71 s, total: 6min 41s\n",
      "Wall time: 6min 45s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv='warn', error_score='raise-deprecating',\n",
       "       estimator=SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,\n",
       "  decision_function_shape='ovr', degree=3, gamma='auto_deprecated',\n",
       "  kernel='rbf', max_iter=-1, probability=False, random_state=None,\n",
       "  shrinking=True, tol=0.001, verbose=False),\n",
       "       fit_params=None, iid='warn', n_jobs=None,\n",
       "       param_grid={'kernel': ('linear', 'rbf'), 'C': [1, 10]},\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score='warn',\n",
       "       scoring=None, verbose=0)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "svc_grid.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'cv': 'warn',\n",
       " 'error_score': 'raise-deprecating',\n",
       " 'estimator__C': 1.0,\n",
       " 'estimator__cache_size': 200,\n",
       " 'estimator__class_weight': None,\n",
       " 'estimator__coef0': 0.0,\n",
       " 'estimator__decision_function_shape': 'ovr',\n",
       " 'estimator__degree': 3,\n",
       " 'estimator__gamma': 'auto_deprecated',\n",
       " 'estimator__kernel': 'rbf',\n",
       " 'estimator__max_iter': -1,\n",
       " 'estimator__probability': False,\n",
       " 'estimator__random_state': None,\n",
       " 'estimator__shrinking': True,\n",
       " 'estimator__tol': 0.001,\n",
       " 'estimator__verbose': False,\n",
       " 'estimator': SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,\n",
       "   decision_function_shape='ovr', degree=3, gamma='auto_deprecated',\n",
       "   kernel='rbf', max_iter=-1, probability=False, random_state=None,\n",
       "   shrinking=True, tol=0.001, verbose=False),\n",
       " 'fit_params': None,\n",
       " 'iid': 'warn',\n",
       " 'n_jobs': None,\n",
       " 'param_grid': {'kernel': ('linear', 'rbf'), 'C': [1, 10]},\n",
       " 'pre_dispatch': '2*n_jobs',\n",
       " 'refit': True,\n",
       " 'return_train_score': 'warn',\n",
       " 'scoring': None,\n",
       " 'verbose': 0}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svc_grid.get_params(deep=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "svc = SVC(C=svc_grid.get_params(deep=True)['estimator__C'], kernel=svc_grid.get_params(deep=True)['estimator__kernel'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,\n",
       "  decision_function_shape='ovr', degree=3, gamma='auto_deprecated',\n",
       "  kernel='rbf', max_iter=-1, probability=False, random_state=None,\n",
       "  shrinking=True, tol=0.001, verbose=False)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svc.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logging training parameters to AzureML and MLFlow experiments\n",
    "run.log(\"C\", svc_grid.get_params(deep=True)['estimator__C'])\n",
    "run.log(\"Kernel\", svc_grid.get_params(deep=True)['estimator__kernel'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_svc = svc.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc = accuracy_score(y_test, predicted_svc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "fscore = f1_score(y_test, predicted_svc, average=\"macro\")\n",
    "precision = precision_score(y_test, predicted_svc, average=\"macro\")\n",
    "recall = recall_score(y_test, predicted_svc, average=\"macro\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import git\n",
    "repo = git.Repo(search_parent_directories=True)\n",
    "sha = repo.head.object.hexsha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Log to AzureML and MLflow\n",
    "run.log(\"Test_accuracy\", acc)\n",
    "run.log(\"Precision\", precision)\n",
    "run.log(\"Recall\", recall)\n",
    "run.log(\"F-Score\", fscore)\n",
    "run.log(\"Git-sha\", sha)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "run id: 5f2d4100-19aa-4ddc-9bf5-402117da8c77\n"
     ]
    }
   ],
   "source": [
    "run.complete()\n",
    "print (\"run id:\", run.id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlflow.end_run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'dataset name': 'training_dataset',\n",
       " 'dataset Version': 1,\n",
       " 'C': 1.0,\n",
       " 'Kernel': 'rbf',\n",
       " 'Test_accuracy': 0.9519180922757906,\n",
       " 'Precision': 0.8869828453699851,\n",
       " 'Recall': 0.8859050416892464,\n",
       " 'F-Score': 0.8864428755463128,\n",
       " 'Git-sha': 'bb282af9afe9422cb0986c873bf881e1c994e580'}"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "run.get_metrics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': '/subscriptions/6faa9ede-4786-48dc-9c1e-0262e2844ebf/resourceGroups/Learn_MLOps/providers/Microsoft.MachineLearningServices/workspaces/MLOps_WS',\n",
       " 'name': 'MLOps_WS',\n",
       " 'location': 'northeurope',\n",
       " 'type': 'Microsoft.MachineLearningServices/workspaces',\n",
       " 'tags': {},\n",
       " 'sku': 'Basic',\n",
       " 'workspaceid': 'e8674bfa-cb69-4989-8a02-de19862bbbd0',\n",
       " 'description': '',\n",
       " 'friendlyName': 'MLOps_WS',\n",
       " 'creationTime': '2020-07-29T05:13:13.4348343+00:00',\n",
       " 'keyVault': '/subscriptions/6faa9ede-4786-48dc-9c1e-0262e2844ebf/resourcegroups/learn_mlops/providers/microsoft.keyvault/vaults/mlopsws6106784693',\n",
       " 'applicationInsights': '/subscriptions/6faa9ede-4786-48dc-9c1e-0262e2844ebf/resourcegroups/learn_mlops/providers/microsoft.insights/components/mlopsws8072806275',\n",
       " 'identityPrincipalId': '4f27e823-96d9-4f95-8782-8f5a34535ad1',\n",
       " 'identityTenantId': 'd22095da-de0a-479f-adc4-47380feb19a1',\n",
       " 'identityType': 'SystemAssigned',\n",
       " 'storageAccount': '/subscriptions/6faa9ede-4786-48dc-9c1e-0262e2844ebf/resourcegroups/learn_mlops/providers/microsoft.storage/storageaccounts/mlopsws3826143660',\n",
       " 'hbiWorkspace': False,\n",
       " 'discoveryUrl': 'https://northeurope.experiments.azureml.net/discovery',\n",
       " 'notebookInfo': {'fqdn': 'ml-mlops_ws-northeurope-e8674bfa-cb69-4989-8a02-de19862bbbd0.notebooks.azure.net',\n",
       "  'resource_id': 'd1a408e9631e4ecea24ff63427e37e94'}}"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "workspace.get_details()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020/08/11 06:20:19 WARNING mlflow.models.model: Logging model metadata to the tracking server has failed, possibly due older server version. The model artifacts have been logged successfully under azureml://experiments/support-vector-machine/runs/9a99e035-1323-4ebf-87c9-98d8548fce05/artifacts. In addition to exporting model artifacts, MLflow clients 1.7.0 and above attempt to record model metadata to the  tracking store. If logging to a mlflow server via REST, consider  upgrading the server version to MLflow 1.7.0 or above.\n"
     ]
    }
   ],
   "source": [
    "import mlflow.sklearn\n",
    "mlflow.sklearn.log_model(svc, 'outputs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Random Forest classifier "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "myexperiment = Experiment(workspace, \"random-forest-classifier\")\n",
    "mlflow.set_experiment(\"mlflow-random-forest-classifier\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf = RandomForestClassifier(max_depth=10, random_state=0, n_estimators=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize runs in Azureml and mlflow\n",
    "run = myexperiment.start_logging()\n",
    "mlflow.start_run()\n",
    "\n",
    "\n",
    "# Log dataset used \n",
    "run.log(\"dataset name\", dataset.name)\n",
    "run.log(\"dataset Version\", dataset.version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 6.52 s, sys: 3.97 ms, total: 6.52 s\n",
      "Wall time: 6.59 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "            max_depth=10, max_features='auto', max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=1, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, n_estimators=100, n_jobs=None,\n",
       "            oob_score=False, random_state=0, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "rf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logging training parameters to AzureML and MLFlow experiments\n",
    "run.log(\"max_depth\", 10)\n",
    "run.log(\"random_state\", 0)\n",
    "run.log(\"n_estimators\", 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_rf = rf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc = accuracy_score(y_test, predicted_rf)\n",
    "fscore = f1_score(y_test, predicted_rf, average=\"macro\")\n",
    "precision = precision_score(y_test, predicted_rf, average=\"macro\")\n",
    "recall = recall_score(y_test, predicted_rf, average=\"macro\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "run.log(\"Test_accuracy\", acc)\n",
    "run.log(\"Precision\", precision)\n",
    "run.log(\"Recall\", recall)\n",
    "run.log(\"F-Score\", fscore)\n",
    "run.log(\"Git-sha\", sha)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "run id: 5596d473-fa15-4925-ab9b-8155845b560b\n"
     ]
    }
   ],
   "source": [
    "run.complete()\n",
    "print (\"run id:\", run.id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlflow.end_run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'dataset name': 'training_dataset',\n",
       " 'dataset Version': 1,\n",
       " 'max_depth': 10,\n",
       " 'n_estimators': 100,\n",
       " 'random_state': 0,\n",
       " 'Test_accuracy': 0.9548989113530326,\n",
       " 'Precision': 0.9018705246237031,\n",
       " 'Recall': 0.8804084310202218,\n",
       " 'F-Score': 0.8907272822498857,\n",
       " 'Git-sha': 'bb282af9afe9422cb0986c873bf881e1c994e580'}"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "run.get_metrics()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Packaging Step"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "pickle file or onnx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING - The maximum opset needed by this model is only 1.\n"
     ]
    }
   ],
   "source": [
    "# Convert into SVC model into ONNX format file\n",
    "from skl2onnx import convert_sklearn\n",
    "from skl2onnx.common.data_types import FloatTensorType\n",
    "initial_type = [('float_input', FloatTensorType([None, 6]))]\n",
    "onx = convert_sklearn(svc, initial_types=initial_type)\n",
    "with open(\"outputs/svc.onnx\", \"wb\") as f:\n",
    "    f.write(onx.SerializeToString())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING - The maximum opset needed by this model is only 1.\n",
      "WARNING - The maximum opset needed by this model is only 9.\n"
     ]
    }
   ],
   "source": [
    "# Convert into RF model into ONNX format file\n",
    "from skl2onnx import convert_sklearn\n",
    "from skl2onnx.common.data_types import FloatTensorType\n",
    "initial_type = [('float_input', FloatTensorType([None, 6]))]\n",
    "onx = convert_sklearn(rf, initial_types=initial_type)\n",
    "with open(\"outputs/rf.onnx\", \"wb\") as f:\n",
    "    f.write(onx.SerializeToString())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Registering Step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Registering model support-vector-classifier\n",
      "Name: support-vector-classifier\n",
      "Version: 1\n"
     ]
    }
   ],
   "source": [
    "# Register Model on AzureML WS\n",
    "model = Model.register(model_path = './outputs/svc.onnx', # this points to a local file \n",
    "                       model_name = \"support-vector-classifier\", # this is the name the model is registered as\n",
    "                       tags = {'dataset': dataset.name, 'version': dataset.version, 'hyparameter-C': '1', 'testdata-accuracy': '0.9519'}, \n",
    "                       model_framework='pandas==0.23.4',\n",
    "                       description = \"Support vector classifier to predict weather at port of Turku\",\n",
    "                       workspace = workspace)\n",
    "\n",
    "print('Name:', model.name)\n",
    "print('Version:', model.version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Registering model random-forest-classifier\n",
      "Name: random-forest-classifier\n",
      "Version: 1\n"
     ]
    }
   ],
   "source": [
    "# Register Model on AzureML WS\n",
    "model = Model.register(model_path = './outputs/rf.onnx', # this points to a local file \n",
    "                       model_name = \"random-forest-classifier\", # this is the name the model is registered as\n",
    "                       tags = {'dataset': dataset.name, 'version': dataset.version, 'hyparameter-C': '1', 'testdata-accuracy': '0.9548'}, \n",
    "                       model_framework='pandas==0.23.4',\n",
    "                       description = \"Random forest classifier to predict weather at port of Turku\",\n",
    "                       workspace = workspace)\n",
    "\n",
    "print('Name:', model.name)\n",
    "print('Version:', model.version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlflow.sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020/08/11 10:59:46 WARNING mlflow.models.model: Logging model metadata to the tracking server has failed, possibly due older server version. The model artifacts have been logged successfully under azureml://experiments/mlflow-random-forest-classifier/runs/f1f99844-efc2-4e48-bced-53c205fb5543/artifacts. In addition to exporting model artifacts, MLflow clients 1.7.0 and above attempt to record model metadata to the  tracking store. If logging to a mlflow server via REST, consider  upgrading the server version to MLflow 1.7.0 or above.\n"
     ]
    }
   ],
   "source": [
    "# Save the model to the outputs directory for capture\n",
    "mlflow.sklearn.log_model(svc, 'outputs/svc.onnx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020/08/11 11:00:21 WARNING mlflow.models.model: Logging model metadata to the tracking server has failed, possibly due older server version. The model artifacts have been logged successfully under azureml://experiments/mlflow-random-forest-classifier/runs/f1f99844-efc2-4e48-bced-53c205fb5543/artifacts. In addition to exporting model artifacts, MLflow clients 1.7.0 and above attempt to record model metadata to the  tracking store. If logging to a mlflow server via REST, consider  upgrading the server version to MLflow 1.7.0 or above.\n"
     ]
    }
   ],
   "source": [
    "# Save the model to the outputs directory for capture\n",
    "mlflow.sklearn.log_model(rf, 'outputs/rf.onnx')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save model artefacts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open('./outputs/scaler.pkl', 'wb') as scaler_pkl:\n",
    "    pickle.dump(sc, scaler_pkl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Registering model scaler\n",
      "Name: scaler\n",
      "Version: 1\n"
     ]
    }
   ],
   "source": [
    "# Register Model on AzureML WS\n",
    "scaler = Model.register(model_path = './outputs/scaler.pkl', # this points to a local file \n",
    "                       model_name = \"scaler\", # this is the name the model is registered as\n",
    "                       tags = {'dataset': dataset.name, 'version': dataset.version}, \n",
    "                       model_framework='pandas==0.23.4',\n",
    "                       description = \"Scaler used for scaling incoming inference data\",\n",
    "                       workspace = workspace)\n",
    "\n",
    "print('Name:', scaler.name)\n",
    "print('Version:', scaler.version)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
